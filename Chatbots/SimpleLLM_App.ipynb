{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "832f3657-ffd7-42e4-8418-1b447823ac77",
   "metadata": {},
   "source": [
    "## Build a simple LLM application with chat models and prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb0a524c-57b4-4def-a24a-666b75d81c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fca2024-3889-41e2-a9d8-813ef35d326b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f96628cb-1373-4821-82d4-509025eaeae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.genai.client.Client at 0x2a29b3fd670>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from google import genai\n",
    "\n",
    "# Access the environment variable\n",
    "genai.Client(api_key=os.environ.get(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963280c9-ebba-4f25-a278-e3758f8e4195",
   "metadata": {},
   "source": [
    "### Using Language Models\n",
    "\n",
    "First up, let's learn how to use a language model by itself. **LangChain** supports many different language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a99f016-749f-498e-9753-649285f08ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model = \"gemini-2.0-flash-lite-preview-02-05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b98ed4-43ef-4d18-97a7-8a8f03ff05df",
   "metadata": {},
   "source": [
    "Let's first use the model directly. `ChatModels` are instances of LangChain `Runnables`, which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of messages to the `.invoke` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29ea6b90-ac12-4211-bb4e-abc17b5f8c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Translate the user sentence to French!\"),\n",
    "    HumanMessage(\"Hi!, My name is Sarath Rachuri\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82ca2ef4-d63f-4d35-9dcf-a04078796d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Bonjour ! Je m'appelle Sarath Rachuri.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-35498bae-0ece-408c-bd28-ea887100b215-0', usage_metadata={'input_tokens': 17, 'output_tokens': 11, 'total_tokens': 28, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86abbc6-e7d1-49c8-99d4-391c22e1836e",
   "metadata": {},
   "source": [
    "LangChain also supports chat model inputs via strings or OpenAI format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bc5226a-435d-4cda-b744-17b85e6cfd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ciao', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-714e1aff-0b41-497b-9901-a545694e7581-0', usage_metadata={'input_tokens': 9, 'output_tokens': 1, 'total_tokens': 10, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke([{\"role\": \"assistant\", \"content\": \"Translate the user sentence into Italian\"}, {\"role\": \"user\", \"content\": \"Hello\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6cc5e-8d30-4ffb-90e8-e9c272d55ccc",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "Since chat models are **Runnables**, they expose a standard interface that includes `async` and `streaming` modes of invocation. Let's stream individual tokens from a chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f2b7641-c428-41c4-aaa8-7003380aeaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour| ! Je m'appelle Sarath Rachuri.|"
     ]
    }
   ],
   "source": [
    "for token in llm.stream(messages):\n",
    "    print(token.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de62e425-642f-4cba-b3fb-5e78c688d3dc",
   "metadata": {},
   "source": [
    "### Prompt Templates\n",
    "\n",
    "Currently, we pass a list of messages directly into the language model. But in real world applications, messages are built from user input and application logic, which transforms raw input into a format suitable for the model.\n",
    "\n",
    "**Prompt templates** in LangChain simplify this process. They take raw user input and return a formatted prompt ready for the language model.\n",
    "\n",
    "Let's create a prompt template that uses two variables:\n",
    "- `language`: The language to translate text into\n",
    "- `text`: The text to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77663011-d1be-43e5-97ec-e51b3d4deef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[AIMessage(content='Translate the user text from English into Spanish', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hi, How are you', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the user text from English into {language}\"\n",
    "\n",
    "# using tuples\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"assistant\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "# using Dictionary\n",
    "prompt = prompt_template.invoke({\"language\": \"Spanish\", \"text\": \"Hi, How are you\"})\n",
    "\n",
    "# to check prompt object\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a84f864e-d4c9-47a5-b288-799b92f5c60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='Translate the user text from English into Spanish', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Hi, How are you', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To access the messages directly\n",
    "prompt.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9348b9f1-5461-492c-955c-e90cf4965d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola, ¿cómo estás?\n"
     ]
    }
   ],
   "source": [
    "# finally, invoke the chat model\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
